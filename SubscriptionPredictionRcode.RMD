---
title: "R Notebook"
output:
  html_document:
    df_print: paged
runtime: shiny
---
#Name: Apoorv Dudhe
#Term: Spring 2019
#Topic:To predict the Probability of a customer subscribing to a term deposit.

#Acquiring the dataset
```{r}
Bank_Marketing <- read.csv("C:/Users/apoor/Desktop/Spring/Intro to Machine Learning/Final Project/bank_Marketing.csv")
Bank_Market <- read.csv("C:/Users/apoor/Desktop/Spring/Intro to Machine Learning/Final Project/bank_Marketing.csv", stringsAsFactors = F)
Bank_Market$y <- as.factor(Bank_Market$y)
summary(Bank_Marketing)
str(Bank_Marketing)      #As we can see that loan, housing,default,martial has "unknown" values. And also poutcome has "nonexistent" values.

#As we can see that Dataset comprises of 21 features: 20 independent variables which has 
#a)10 categorical features - Job, Martial, education, default, housing , loan , contact , month, day_of_week, poutcome 
#b) 10 numeric features - age, duration, campaign , pdays , previous ,  emp.var.rate ,cons.price.idx, cons.conf.idx,  euribor3m ,nr.employed

```
#Analysing the features
```{r}
#Since the subscription of term depends more on the duration, Hence,

duration <- which(Bank_Market$duration==0)
Bank_Market[duration,]                       #We can see that there are 4 values that have a duration of 0 which says that they dint subscribe the term. 
duration1<- which(Bank_Market$duration<80)
Bank_Market[duration1,]                      #If we test for values less than 80 then there are about 6896 customers who did not subscribe for the term.

#Hence we get biased decision for less number of duration. We will build a model with and without this feature and compare the performance.
```
#Outlier detection
```{r}
library(lattice)
library(DMwR)
library(variables)
hist(Bank_Marketing$age)                   #We can see in this that age has some outlier values but they are not any random values hence we need not replace them.
Bank = boxplot(Bank_Marketing$age)$out
z<-abs(scale(Bank_Marketing$age,center = TRUE,scale=TRUE))
outlier_a<-which(z>5)
str(outlier_a)
Non_outlier_a<-Bank_Marketing[outlier_a,]
nrow(Non_outlier_a)
n<-nrow(Bank_Marketing)
Marketing_outlier<-Bank_Marketing[,c("age","euribor3m","duration","emp.var.rate","nr.employed","pdays")]
outlier <- lm(age~ ., data=Marketing_outlier)
Dist <- cooks.distance(outlier)
plot(Dist, pch="*", cex=2, main="Influential Obs by Cooks distance")


#visualize outlier
pch <- rep(".", n)
col <- rep("black", n)
pairs(Marketing_outlier, pch=pch, col=col)

#pdays feature analysis
pdays_ana<-which(Bank_Marketing$pdays==999)
str(pdays_ana)
hist(Bank_Marketing$pdays)
#heavily skewed towards left
#applying some transformations
hist(log(Bank_Marketing$pdays+4))
hist(sqrt(Bank_Marketing$pdays))

#The data is skewwed minimally since the features have some outliers but these are not random values and it is possible to have such values in real world hence we will keep the outlier values as it is and it won't affect our model that much.
#Majority of values in pdays column  equal to 999 which means there was no previous contact hence it is better to eliminate this feature from our analysis.For example age has outlier values above 75 years of age in the dataset. It is possible to have the age of a client to be 75 or above.


```
#Imputing Missing data using mice package
```{r}
#We have to replacing "unknown" "nonexitent" cases and impute them.
impute<-read.csv("C:/Users/apoor/Desktop/Spring/Intro to Machine Learning/Final Project/bank_Marketing.csv",na.strings = c("unknown","nonexistent"))
#numeric features
impute_numerical<-as.data.frame(impute[c('age','day_of_week','duration','campaign','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed')])
#categorical features
impute_categorical<-as.data.frame(impute[c(2,3,4,5,6,7,8,9,10,15,21)])
library(mice)
md.pattern(impute)
impute$y<-as.factor(ifelse(impute$y=="no",0,1))

library(VIM)
mice_plot <- aggr(impute, col=c('red','black'),
                    numbers=TRUE, sortVars=TRUE,
                    labels=names(impute), cex.axis=.7,
                    gap=3, ylab=c("Missing data","Pattern"))
imputed_d <- mice(impute, m=5, maxit = 5, method = 'pmm', seed = 500)
model_imputed<-complete(imputed_d,2)                     #We have imputed the model using mice 

```

#Eliminating pdays  feature
```{r}
Bank_Marketing<-Bank_Marketing[-13]
#removing illiterate value from education feature as there is only one value
Bank_Market<-Bank_Market[-13]

#segmenting features on their type to apply transformations for different models
bank_numerical<-as.data.frame(Bank_Marketing[c('age','campaign','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed')])
bank_numerical_1<-as.data.frame(Bank_Market[c('age','campaign','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed')])
bank_categorical<-as.data.frame(Bank_Marketing[c(2,3,4,5,6,7,8,9,10,13)])
bank_categorical_1<-as.data.frame(Bank_Market[c(2,3,4,5,6,7,8,9,10,13)])
diy<-as.factor(Bank_Marketing$y) #target variable
diy2<-as.factor(Bank_Marketing$y)

```
#Normalising the numeric feature
```{r}

normalization <- function(x) {
return ((x - min(x)) / (max(x) - min(x))) }               #normalizing the numeric features using Min-Max method
Normalized_Marketing <- as.data.frame(lapply(bank_numerical, normalization))
Normalized_Marketing_1<-as.data.frame(lapply(bank_numerical_1, normalization))




library(ade4)
bank_dummyvars<- acm.disjonctif(bank_categorical)             #dummy coding categorical feature
bank_dummyvars_1<-acm.disjonctif(bank_categorical_1)
decision<-as.factor(ifelse(Bank_Marketing$y=="yes",1,0))
Dummy_coded_data<-cbind(Normalized_Marketing,bank_dummyvars,decision)
str(Dummy_coded_data)
```
#Data correlation/collinearity
```{r}
dec <- ifelse(Bank_Marketing$y=="yes",1,0)
library(psych)
a<-cbind(dec,Normalized_Marketing)
pairs.panels(a) #correlation with numeric feature
b<-cbind(dec,bank_categorical)
pairs.panels(b) #correlation with categorical features

#As we can see that some of the numerical features have significant collinearity
#1) employee variance rate and consumer price index- 0.78
#2) employee variance rate and euribor- 0.97
#3) euribor and number of employees - 0.95

#As  categorical features have factored values we do not see much collinearity.Also, with using dummy coded variables we cannot get a significant answer for that specific feature.There are 12 variables for job feature as it has 12 different classes. Hence if we use dummy variables the plot won't show the correlation for "job" as a feature. 
```
#PCA
```{r}
#PCA has been performed on numeric features since the dataset also contains categorical features.
q<-cbind(bank_numerical,dec)
pca<-princomp(q,scores = TRUE,cor=TRUE)
loadings(pca)
plot(pca)
biplot(pca)
pca$scores
```
#Creating new feature column
```{r}

#We will be creating three classes for age i.e.young, middle aged and senior citizen

table(Bank_Marketing$age)
new_feature_age<-ifelse(Bank_Marketing$age<35,"young",Bank_Marketing$age)
new_feature_age_1<-ifelse(Bank_Marketing$age>=35 & Bank_Marketing$age <60,"middle aged",new_feature_age)
new_feature_age_2 <- ifelse (Bank_Marketing$age>=60,"Senior citizen",new_feature_age_1)
new_feature_age_2
```
#data Partition
```{r}
#dataset without normalization
library(caret)
set.seed(1111)
Index <- createDataPartition(Bank_Marketing$y, p=0.60, list = FALSE)  
train_Marketing<-Bank_Marketing[Index,]
Validation_Marketing<-Bank_Marketing[-Index,]
y_not<-ifelse(Validation_Marketing$y=="yes",1,0)

#dataset with normalization 
set.seed(1111)
Index_1<-createDataPartition(Dummy_coded_data$decision,p=0.60,list=F)
train_Marketing_1<-Dummy_coded_data[Index_1,]
Validation_Marketing_1<-Dummy_coded_data[-Index_1,]
prop.table(table(train_Marketing_1$decision)) 
prop.table(table(Validation_Marketing_1$decision))

set.seed(1111)
indexx<-createDataPartition(Bank_Market$y,p=0.60,list=F)
train_Market<-Bank_Market[indexx,]
Valid_Market<-Bank_Market[-indexx,]


#we can use the above dataset for logistic regression. Also, we dont need to dummy code categorical values since in logistic regression it itself dummy codes the categorical values.
```
#Building a logistic regression model
```{r}
library(pROC)
logistic_regression_model<-glm(formula=y~ .,data=train_Marketing,family=binomial)
summary(logistic_regression_model)
#backward fitting the model with AIC
logistic_regression_model<-step(glm(formula=y~ .,data=train_Marketing,family=binomial),direction = "backward")
anova(logistic_regression_model, test = 'Chisq')
plot(logistic_regression_model)
prediction_logistic<-predict(logistic_regression_model,Validation_Marketing,type="response")
prediction_logistic<-ifelse(prediction_logistic>0.40,1,0)
table(prediction_logistic,y_not)
plot(prediction_logistic)
confusionMatrix(factor(prediction_logistic),factor(y_not))
auc_log<-auc(as.numeric(prediction_logistic),as.numeric(y_not))
auc_log

#In this model we can see that the accuracy given by the confusion matrix is 0.909.
#Kappa value turns out to be 0.506
#Area under the curve is 0.778
```

```{r}
m_knn<-train(y~.,data=train_Marketing,method="knn",metric="Kappa",trControl=trainControl(method = "cv",number=10),tuneGrid=expand.grid(.k=c(50,55,60)))                                #k-fold cross validation
plot(m_knn)
predict_knn<-predict(m_knn,Validation_Marketing)
table(predict_knn,Validation_Marketing$y)
confusionMatrix(predict_knn,Validation_Marketing$y)

auc_knn<-auc(as.numeric(predict_knn),as.numeric(y_not))
auc_knn
#In this model we can see that the accuracy given by the confusion matrix is 0.91.
#Kappa value turns out to be 0.49
#Area under the curve is 0.778

m_knn1<-train(y~.,data=train_Marketing,method="knn",metric="Kappa",trControl=trainControl(method = "cv",number=14),tuneGrid=expand.grid(.k=c(50,55,60)))

predict_knn1<-predict(m_knn1,Validation_Marketing)
table(predict_knn1,Validation_Marketing$y)
plot(predict_knn1)
confusionMatrix(predict_knn,Validation_Marketing$y)

auc_knn1<-auc(as.numeric(predict_knn),as.numeric(y_not))
auc_knn1
#In this model we can see that the accuracy given by the confusion matrix is 0.91.
#Kappa value turns out to be 0.4917
#Area under the curve is 0.7857
```

```{r}
set.seed(1005)
svm_model<-train(y~.,data=train_Marketing,method="svmRadial",trControl=trainControl(method='cv',number=10)) #k-fold cross validation model
svm_predict<-predict(svm_model,Validation_Marketing)
table(svm_predict,Validation_Marketing$y)
confusionMatrix(svm_predict,Validation_Marketing$y)
plot(svm_predict)
auc_svm<-auc(as.numeric(svm_predict),as.numeric(y_not))
auc_svm
plot(svm_model)
#In this model we can see that the accuracy given by the confusion matrix is 0.9059
#Kappa value turns out to be 0.4136
#Area under the curve is 0.786




```
#Comparison of models
```{r}
#Comparison of all the 3 models can be done on the basis of Accuracy, Kappa values and Auc
#For logistic model accuracy = 0.909   kappa = 0.506  AUC = 0.778
#For Knn            accuracy = 0.91    kappa = 0.49   AUC = 0.778
#For SVM            accuracy = 0.9059  Kappa = 0.4136 AUC = 0.786
```

#R shiny model

```{r}
library(shiny)
library(shinydashboard)


ui <- fluidPage(

  
  titlePanel("Uploading Files"),
  sidebarLayout(

    
    sidebarPanel(

      fileInput("file1", "Choose CSV File",
                multiple = FALSE,
                accept = c("text/csv",
                         "text/comma-separated-values,text/plain",
                         ".csv")),
      tags$hr(),

      checkboxInput("header", "Header", TRUE),

      
      radioButtons("sep", "Separator",
                   choices = c(Comma = ",",
                               Semicolon = ";")),
                   
      
      radioButtons("quote", "Quote",
                   choices = c(None = "",
                               "Double Quote" = '"',
                               "Single Quote" = "'"),
                   selected = '"'),

      
      tags$hr(),

      
      radioButtons("disp", "Display",
                   choices = c(Head = "head",
                               All = "all"),
                   selected = "head")

    ),

    
    mainPanel(

      
      tableOutput("contents")

    )

  )
)


server <- function(input, output) {

  output$contents <- renderTable({
    

    req(input$file1)

   
    tryCatch(
      {
        df <- read.csv(input$file1$datapath,
                 header = input$header,
                 sep = input$sep,
                 quote = input$quote)
      },
      error = function(e) {
        
        stop(safeError(e))
      }
    )

    if(input$disp == "head") {
      return(head(df))
    }
    else {
      return(df)
    }   

  })

}
  


shinyApp(ui, server)
```

```

